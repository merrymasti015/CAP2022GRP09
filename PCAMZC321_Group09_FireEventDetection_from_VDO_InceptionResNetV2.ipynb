{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/merrymasti015/CAP2022GRP09/blob/main/PCAMZC321_Group09_FireEventDetection_from_VDO_InceptionResNetV2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mOulgwdAWmq"
      },
      "source": [
        "### Apply model on vdo having no class\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZ-t22ygAWms"
      },
      "source": [
        "### Apply model on vdo having no class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjbkLKFcAWms",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe69a32f-51d2-4fbc-d590-28c27a211c72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imageio: 'ffmpeg-linux64-v3.3.1' was not found on your computer; downloading it now.\n",
            "Try 1. Download from https://github.com/imageio/imageio-binaries/raw/master/ffmpeg/ffmpeg-linux64-v3.3.1 (43.8 MB)\n",
            "Downloading: 8192/45929032 bytes (0.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2187264/45929032 bytes (4.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5038080/45929032 bytes (11.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b8028160/45929032 bytes (17.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11034624/45929032 bytes (24.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13950976/45929032 bytes (30.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16990208/45929032 bytes (37.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20234240/45929032 bytes (44.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b23142400/45929032 bytes (50.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b26443776/45929032 bytes (57.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b29163520/45929032 bytes (63.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31842304/45929032 bytes (69.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b34398208/45929032 bytes (74.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b37224448/45929032 bytes (81.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b40148992/45929032 bytes (87.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b43319296/45929032 bytes (94.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45929032/45929032 bytes (100.0%)\n",
            "  Done\n",
            "File saved as /root/.imageio/ffmpeg/ffmpeg-linux64-v3.3.1.\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.layers import Dense, InputLayer, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "\n",
        "\n",
        "import cv2\n",
        "import math\n",
        "import os, os.path\n",
        "from pathlib import Path\n",
        "from glob import glob\n",
        "from scipy import stats as s\n",
        "\n",
        "from moviepy.editor import *   ###VideoFileClip\n",
        "\n",
        "from ipywidgets import Video # Display VDO \n",
        "\n",
        " \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnJovgK1AWmu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16de42fc-6b64-4721-ad50-d3ac9f22cc53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/MyDrive/Fire_ImageDataSet/ModelSave\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "ModelSaveFolder = \"/content/gdrive/MyDrive/Fire_ImageDataSet/ModelSave\"\n",
        "\n",
        "\n",
        "Drive = \"/content/gdrive/MyDrive/\"\n",
        "## Define root folder\n",
        "RootFolder = Drive+\"Fire_ImageDataSet/OutputFiles\"\n",
        "\n",
        "### Where to save models\n",
        "\n",
        "ModelSaveFolder = Drive+\"Fire_ImageDataSet/ModelSave\"\n",
        "\n",
        "print(ModelSaveFolder)\n",
        "\n",
        "VDOFolder = Drive+\"/Fire_ImageDataSet/VDO_fire\"  # where is my VDO file ?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wBel3CSAWmu",
        "outputId": "7f4320b5-efe7-4187-9d36-be997e3cd233",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/MyDrive//Fire_ImageDataSet/VDO_fire/VDOFrames1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "FramesVdo  = VDOFolder+\"/VDOFrames1\"\n",
        "if not os.path.exists(FramesVdo):\n",
        "   os.makedirs(FramesVdo) \n",
        "FramesVdo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsQfb3CEAWmv",
        "outputId": "3ab6c212-9be1-4b9b-f5c8-5e00d08e4ff5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not empty\n"
          ]
        }
      ],
      "source": [
        "\n",
        "if not os.listdir(FramesVdo):\n",
        "    print (\"empty\")\n",
        "else:\n",
        "    print (\"Not empty\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fb8NEb4OAWmw",
        "outputId": "4403e01a-d6f9-4279-d94b-05821580233f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Fire', 'No Fire']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Numeric to semantic labels \n",
        "model_class = ['Fire', 'No Fire']\n",
        "model_class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBgRP4i0AWmw",
        "outputId": "07b1cc11-a60c-480a-e224-0aa73770636e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Fire'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "model_class[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOxkAiUxAWmw"
      },
      "source": [
        "## Pre Processing "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkAOTVJwAWmw",
        "outputId": "1bcdbfd2-227c-46e2-e57f-6c8473f1bd4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/MyDrive//Fire_ImageDataSet/VDO_fire/VDO-1.mp4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "\n",
        "video_title = \"VDO-1\"\n",
        "\n",
        "input_video_file_path = f'{VDOFolder}/{video_title}.mp4'\n",
        "input_video_file_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dOiy2wpAWmx",
        "outputId": "08a5a244-1c4e-45cf-d5d4-1a8dafe7655b",
        "colab": {
          "referenced_widgets": [
            "726744dc578f47c7980822d6ca71e9cd",
            "0ac4efcf5f10452781a7d5613e73e84f"
          ],
          "base_uri": "https://localhost:8080/",
          "height": 205
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Video(value=b'\\x00\\x00\\x00 ftypisom\\x00\\x00\\x02\\x00isomiso2avc1mp41\\x00\\x00\\x00\\x08free\\x00\\xe3\\xaa_mdat\\x00\\xâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "726744dc578f47c7980822d6ca71e9cd"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Video.from_file(input_video_file_path)\n",
        "Video.from_file(input_video_file_path , width = 320 , height = 320)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDOzSBugAWmx"
      },
      "outputs": [],
      "source": [
        "from ipywidgets import Widget\n",
        "Widget.close_all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xugg9ZQFAWmx"
      },
      "outputs": [],
      "source": [
        "def Pre_Processing(input_video_file_path,FramesVdo):\n",
        "    \n",
        "    flag =''\n",
        "    image_height  = 150\n",
        "    image_width   = 150\n",
        "    \n",
        "    EXTENSION = \".jpg\"\n",
        "    curr_frame = 0\n",
        "    imageKount = 0\n",
        "    KPS = 5 # Target Keyframes Per Second\n",
        "    # Reading the Video File using the VideoCapture Object\n",
        "    video_reader = cv2.VideoCapture(input_video_file_path)\n",
        "    # Set frames-per-second for capture\n",
        "    fps = round(video_reader.get(cv2.CAP_PROP_FPS))\n",
        "    print('FPS = ' ,fps)\n",
        "    hop = round(fps / KPS)\n",
        "    #curr_frame = 5\n",
        "    prediction_images = []\n",
        "    \n",
        "    ## Check if VDOFrames is empty or Not --- If empty then frames will be stored else NOT\n",
        "    ## OR we can delete all files in VDOFrames folder unconditionally \n",
        "    \n",
        "    if not os.listdir(FramesVdo):\n",
        "        print (\"empty VDOFrames folder\")\n",
        "        flag = \"empty\"\n",
        "    else:\n",
        "        print (\"Not empty\")\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    while True: \n",
        "\n",
        "        # Reading The Frame\n",
        "        status, frame = video_reader.read() \n",
        "        \n",
        "        if not status:\n",
        "            break\n",
        "\n",
        "        # Resize the Frame to fixed Dimensions\n",
        "        resized_frame = cv2.resize(frame, (image_height, image_width))\n",
        "        \n",
        "        ## if VDOFrames is empty i.e. flag = \"empty\" then write\n",
        "        \n",
        "        if flag == 'empty':\n",
        "           \n",
        "           temp = Path(input_video_file_path).stem # Find file name without extension\n",
        "           filename =\"/\"+ temp+\"_frame%d.jpg\" % curr_frame\n",
        "           Outimage = os.path.join(FramesVdo+filename)\n",
        "           cv2.imwrite(Outimage, resized_frame)   # save frame as JPEG file\n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "        ## PreProcessing  \n",
        "        normalized_frame = resized_frame / 255\n",
        "        # appending the image to the image list\n",
        "        prediction_images.append(normalized_frame)\n",
        "        imageKount +=1\n",
        "        curr_frame +=1\n",
        "      \n",
        "    \n",
        "    # converting the list to numpy array\n",
        "    prediction_images = np.array(prediction_images)\n",
        "        \n",
        "    # shape of the array\n",
        "    print(prediction_images.shape)\n",
        "    print(curr_frame)\n",
        "    print(imageKount)   \n",
        "    #return batch_data\n",
        "    return prediction_images  # Return prediction_images numpy array\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CooEKrgAWmy",
        "outputId": "48c32316-e706-4d16-b144-8664779b3a5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FPS =  25\n",
            "Not empty\n",
            "(1872, 150, 150, 3)\n",
            "1872\n",
            "1872\n",
            "CPU times: user 15.2 s, sys: 1.06 s, total: 16.3 s\n",
            "Wall time: 6.98 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "prediction_images = Pre_Processing(input_video_file_path,FramesVdo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCSw6ILPAWmy"
      },
      "source": [
        "## End of Pre Processing "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPPt1nqAAWmy"
      },
      "source": [
        "## Event Prediction "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3X2u5KWAWmy"
      },
      "source": [
        "### Load  model from disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbBb2a8IAWmy"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Wp23-dCAWmz"
      },
      "outputs": [],
      "source": [
        "#model_name = 'InceptionResNetV2_Prediction_fine_tuned.h5'\n",
        "model_name = 'InceptionResNetV2_suns.h5'\n",
        "modeIV2 = keras.models.load_model(ModelSaveFolder+'/'+model_name)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPPvbNGqAWmz",
        "outputId": "542d6bfd-b943-46a0-a1c5-0a776ce95420",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 ... 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "predict = []        \n",
        "# predict_classes() function model in order to predict the class values for each instance in the array.\n",
        "#prediction = np.round(modeIV2.predict_classes(prediction_images)).flatten()\n",
        "prediction = np.argmax(modeIV2.predict(prediction_images), axis=-1)\n",
        "#np.round(model.predict(img)).flatten()\n",
        "\n",
        "# prediction is an arrary containing frame by frame class prediction - here either Basketball', 'SoccerPenalty'\n",
        "\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhwaSYFtAWmz"
      },
      "source": [
        "#### Pickle is the standard way of serializing objects in Python.\n",
        "\n",
        "#### Use the pickle operation to serialize  machine learning algorithms and save the serialized format to a file.\n",
        "\n",
        "#### Later we can load this file to deserialize your model and use it to make new predictions.\n",
        "\n",
        "#### Serialization is the process of converting an object into a stream of bytes to store the object or transmit it to memory, or a file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7Lr8UTYAWmz"
      },
      "source": [
        "### Save prediction array using pickle dump"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RR7uPidAWm0"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open('IV2Prediction_j', 'wb') as fp:\n",
        "    pickle.dump(prediction, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EWYX6F7AWm0"
      },
      "outputs": [],
      "source": [
        "#To read it back:\n",
        "with open ('IV2Prediction_j', 'rb') as fp:\n",
        "    prediction = pickle.load(fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LShAlWXzAWm0",
        "outputId": "03f6e795-475b-4b76-afd2-b7c9af51605e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "print(prediction[5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-NBVw9NAWm0"
      },
      "source": [
        "### Function To Predict on Live Videos "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cACH2DSmAWm0",
        "outputId": "c9cf8cba-a4d8-4856-c04d-42a1fce25edf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive//Fire_ImageDataSet/VDO_fire/VDO-1.mp4\n",
            "/content/gdrive/MyDrive//Fire_ImageDataSet/VDO_fire/VDOFrames1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(input_video_file_path)\n",
        "print(FramesVdo)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8RxrwoaAWm1",
        "outputId": "1f034fab-3426-4e23-86cb-ba2f2e09175b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "prediction[17]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYs3mueCAWm1",
        "outputId": "7cabb406-1e59-4089-f329-d990486eab4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1872\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Fire'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "x = len(prediction)\n",
        "print(x)\n",
        "# Accessing The Class Name using predicted label.\n",
        "predicted_class_name = model_class[prediction[2]]\n",
        "predicted_class_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzESsYroAWm1",
        "outputId": "92b39920-7f48-46df-de6f-0959b65de0fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/MyDrive//Fire_ImageDataSet/VDO_fire/VDO-1.mp4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "input_video_file_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FeuIzHYnAWm1"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8djk8fryAWm1"
      },
      "source": [
        "#### cv2.putText() method is used to draw a text string on any image.\n",
        "\n",
        "Syntax: cv2.putText(image, text, org, font, fontScale, color[, thickness[, lineType[, bottomLeftOrigin]]])\n",
        "\n",
        "Parameters:\n",
        "image:           It is the image on which text is to be drawn.\n",
        "text:            Text string to be drawn.\n",
        "org:             It is the coordinates of the bottom-left corner of the text string in the image. The coordinates are represented as tuples of two values i.e. (X coordinate value, Y coordinate value).\n",
        "font:            It denotes the font type. Some of font types are FONT_HERSHEY_SIMPLEX, FONT_HERSHEY_PLAIN, , etc.\n",
        "fontScale:       Font scale factor that is multiplied by the font-specific base size.\n",
        "color:           It is the color of text string to be drawn. For BGR, we pass a tuple. eg: (255, 0, 0) for blue color.\n",
        "thickness:       It is the thickness of the line in px.\n",
        "lineType:        This is an optional parameter.It gives the type of the line to be used.\n",
        "bottomLeftOrigin: This is an optional parameter. When it is true, the image data origin is at the bottom-left corner. Otherwise, it is at the top-left corner.\n",
        "\n",
        "Return Value: It returns an image.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r81v92CrAWm1"
      },
      "outputs": [],
      "source": [
        "def predict_on_live_video(video_file_path, output_file_path,Algo):\n",
        "\n",
        "    # Initialize a Deque Object with a fixed size which will be used to implement moving/rolling average functionality.\n",
        "    #predicted_labels_probabilities_deque = deque(maxlen = window_size)\n",
        "    \n",
        "    frame_no = -1\n",
        "\n",
        "    # Reading the Video File using the VideoCapture Object\n",
        "    video_reader = cv2.VideoCapture(video_file_path)\n",
        "\n",
        "    # Getting the width and height of the video \n",
        "    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    print(original_video_width)\n",
        "    print(original_video_height)\n",
        "    # Writing the Overlayed Video Files Using the VideoWriter Object\n",
        "    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 24, (original_video_width, original_video_height))\n",
        "\n",
        "    while True: \n",
        "\n",
        "        # Reading The Frame\n",
        "        status, frame = video_reader.read() \n",
        "\n",
        "        if not status:\n",
        "            break\n",
        "\n",
        "        frame_no = frame_no+1\n",
        "        \n",
        "        \n",
        "        # Using cv2.putText()\n",
        "        \n",
        "        # Accessing The Class Name using prediction list.\n",
        "        predicted_class_name = model_class[prediction[frame_no]]\n",
        "        #print(predicted_class_name)\n",
        "        \n",
        "        # Overlaying Class Name Text Ontop of the Frame\n",
        "        cv2.putText(frame, predicted_class_name+'_(' + Algo + ')', (00, 200), cv2.FONT_HERSHEY_DUPLEX, 3.0, (0, 0, 255), 3)\n",
        "        # Writing The Frame\n",
        "        video_writer.write(frame)\n",
        "    \n",
        "    # Closing the VideoCapture and VideoWriter objects and releasing all resources held by them. \n",
        "    video_reader.release()\n",
        "    video_writer.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "M2DLw5bbAWm2",
        "outputId": "178bcf21-feda-42ec-8c79-d9e92b8ec245",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive//Fire_ImageDataSet/VDO_fire/VDO-1_IV2_Gr_9_Output.mp4\n",
            "1280\n",
            "720\n"
          ]
        }
      ],
      "source": [
        "ModelName = 'IV2'\n",
        "output_video_file_path  = VDOFolder+'/'+video_title+'_'+ ModelName+'_Gr_9_Output.mp4'\n",
        "print(output_video_file_path)\n",
        "# Calling the predict_on_live_video method to start the Prediction.\n",
        "predict_on_live_video(input_video_file_path, output_video_file_path , ModelName)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Uo3MnkvAWm2"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Di-U_8z2AWm2"
      },
      "source": [
        "### End"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kTcA04SAWm2"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "PCAMZC321_Group09_FireEventDetection_from_VDO-InceptionResNetV2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "726744dc578f47c7980822d6ca71e9cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VideoModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VideoModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VideoView",
            "autoplay": true,
            "controls": true,
            "format": "mp4",
            "height": "320",
            "layout": "IPY_MODEL_0ac4efcf5f10452781a7d5613e73e84f",
            "loop": true,
            "width": "320"
          }
        },
        "0ac4efcf5f10452781a7d5613e73e84f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}