{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/merrymasti015/CAP2022GRP09/blob/main/PCAMZC321_Group09_FireEventDetection_from_VDO_VGG16_Feature_Gr_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4tnkjj6A3Du"
      },
      "source": [
        "### Apply model on vdo having no class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_31TcAxA3Dy"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.layers import Dense, InputLayer, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import math\n",
        "import os\n",
        "from glob import glob\n",
        "from scipy import stats as s\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsESYszhA3D0",
        "outputId": "74e1c7ca-e056-4b40-a6a9-78ee04aecd8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/MyDrive/Fire_ImageDataSet/ModelSave\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "ModelSaveFolder = \"/content/gdrive/MyDrive/Fire_ImageDataSet/ModelSave\"\n",
        "\n",
        "\n",
        "Drive = \"/content/gdrive/MyDrive/\"\n",
        "## Define root folder\n",
        "RootFolder = Drive+\"Fire_ImageDataSet/OutputFiles\"\n",
        "\n",
        "### Where to save models\n",
        "\n",
        "ModelSaveFolder = Drive+\"Fire_ImageDataSet/ModelSave\"\n",
        "\n",
        "print(ModelSaveFolder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "0yTZ61kjA3D1",
        "outputId": "f31e04f9-4a9a-4805-e500-fa05404508c3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     FrameFilename                                       FullPathName  \\\n",
              "0   N_image_89.jpg  /content/gdrive/MyDrive/Fire_ImageDataSet//Out...   \n",
              "1  N_image_886.jpg  /content/gdrive/MyDrive/Fire_ImageDataSet//Out...   \n",
              "2  N_image_896.jpg  /content/gdrive/MyDrive/Fire_ImageDataSet//Out...   \n",
              "3   N_image_96.jpg  /content/gdrive/MyDrive/Fire_ImageDataSet//Out...   \n",
              "4   N_image_95.jpg  /content/gdrive/MyDrive/Fire_ImageDataSet//Out...   \n",
              "\n",
              "                                    GreyFullPathName    class  \n",
              "0  /content/gdrive/MyDrive/Fire_ImageDataSet//Out...  Neutral  \n",
              "1  /content/gdrive/MyDrive/Fire_ImageDataSet//Out...  Neutral  \n",
              "2  /content/gdrive/MyDrive/Fire_ImageDataSet//Out...  Neutral  \n",
              "3  /content/gdrive/MyDrive/Fire_ImageDataSet//Out...  Neutral  \n",
              "4  /content/gdrive/MyDrive/Fire_ImageDataSet//Out...  Neutral  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dad31cdc-3cfe-46fd-a024-dbba276411c1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>FrameFilename</th>\n",
              "      <th>FullPathName</th>\n",
              "      <th>GreyFullPathName</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>N_image_89.jpg</td>\n",
              "      <td>/content/gdrive/MyDrive/Fire_ImageDataSet//Out...</td>\n",
              "      <td>/content/gdrive/MyDrive/Fire_ImageDataSet//Out...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>N_image_886.jpg</td>\n",
              "      <td>/content/gdrive/MyDrive/Fire_ImageDataSet//Out...</td>\n",
              "      <td>/content/gdrive/MyDrive/Fire_ImageDataSet//Out...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>N_image_896.jpg</td>\n",
              "      <td>/content/gdrive/MyDrive/Fire_ImageDataSet//Out...</td>\n",
              "      <td>/content/gdrive/MyDrive/Fire_ImageDataSet//Out...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>N_image_96.jpg</td>\n",
              "      <td>/content/gdrive/MyDrive/Fire_ImageDataSet//Out...</td>\n",
              "      <td>/content/gdrive/MyDrive/Fire_ImageDataSet//Out...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>N_image_95.jpg</td>\n",
              "      <td>/content/gdrive/MyDrive/Fire_ImageDataSet//Out...</td>\n",
              "      <td>/content/gdrive/MyDrive/Fire_ImageDataSet//Out...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dad31cdc-3cfe-46fd-a024-dbba276411c1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dad31cdc-3cfe-46fd-a024-dbba276411c1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dad31cdc-3cfe-46fd-a024-dbba276411c1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "trainFrames  = pd.read_csv(RootFolder+\"/Train_FramePathLocation.csv\")\n",
        "\n",
        "trainFrames.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9NZE3jhA3D1",
        "outputId": "59183798-6eb0-4ede-d746-985c74e8b91f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Neutral', 'Fire']\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "# Get the unique values of 'class' column - How many distinct classes\n",
        "model_class = trainFrames['class'].unique().tolist()\n",
        "print(model_class)\n",
        "model_output_size = len(model_class)\n",
        "print(model_output_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7jFAi1zA3D2"
      },
      "source": [
        "## Pre Processing "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3hCyT9xA3D2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21da155e-744b-49b8-9962-a3571f6d173f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "58900480/58889256 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "## VGG-16 pre-trained model will be used\n",
        "# creating the base model of pre-trained VGG16 model\n",
        "base_model = VGG16(weights='imagenet', include_top=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4zO1KWhwA3D3",
        "outputId": "e0ecfcf7-f192-42f1-876a-8b5e08bd414d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/MyDrive//Fire_ImageDataSet/VDO_fire/VDO-1.mp4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "\n",
        "output_directory = Drive+\"/Fire_ImageDataSet/VDO_fire\"  # where is my VDO file ?\n",
        "\n",
        "\n",
        "video_title = \"VDO-1\"\n",
        "\n",
        "input_video_file_path = f'{output_directory}/{video_title}.mp4'\n",
        "input_video_file_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205,
          "referenced_widgets": [
            "7234ff50a8d642009564c62e1a90e3f6",
            "7935f548d115489d9911a6299323ede4"
          ]
        },
        "id": "9oIJpXuQA3D3",
        "outputId": "54b22a8b-a53e-4602-fa4e-37e97d28e6af"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Video(value=b'\\x00\\x00\\x00 ftypisom\\x00\\x00\\x02\\x00isomiso2avc1mp41\\x00\\x00\\x00\\x08free\\x00\\xe3\\xaa_mdat\\x00\\x…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7234ff50a8d642009564c62e1a90e3f6"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Video.from_file(input_video_file_path)\n",
        "from ipywidgets import Video # Display VDO \n",
        "Video.from_file(input_video_file_path , width = 320 , height = 320)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0-SCiEVA3D4"
      },
      "outputs": [],
      "source": [
        "from ipywidgets import Widget\n",
        "Widget.close_all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mMwMVZ4nA3D4",
        "outputId": "e5aa58ea-995b-4768-a10d-906937deee51"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/MyDrive//Fire_ImageDataSet/VDO_fire/VDOFrames'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "FramesVdo  = output_directory+\"/VDOFrames\"\n",
        "if not os.path.exists(FramesVdo):\n",
        "   os.makedirs(FramesVdo) \n",
        "FramesVdo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPkBA2JmA3D4",
        "outputId": "c7dca23e-a112-4907-cc2a-c297338456b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fps = 25\n",
            "number of frames = 1876\n",
            "duration (S) = 75\n",
            "duration (M:S) = 1:15\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Reading the Video File using the VideoCapture Object\n",
        "video_reader = cv2.VideoCapture(input_video_file_path)\n",
        "# Set frames-per-second for capture\n",
        "fps = round(video_reader.get(cv2.CAP_PROP_FPS))\n",
        "## Total frame count in the VDO \n",
        "frame_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT)) -1 \n",
        "\n",
        "\n",
        "duration = round(frame_count/fps)\n",
        "\n",
        "print('fps = ' + str(fps))\n",
        "print('number of frames = ' + str(frame_count))\n",
        "print('duration (S) = ' + str(duration))\n",
        "\n",
        "minutes = int(duration/60)\n",
        "seconds = duration%60\n",
        "print('duration (M:S) = ' + str(minutes) + ':' + str(seconds))  ## In Minute and seconds\n",
        "\n",
        "video_reader.release()\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5PjQWMmA3D5",
        "outputId": "5cd3f0bf-d7b2-4f6e-f57c-68901f1a1b16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "empty VDOFrames folder\n",
            "Shape of prediction_images =  (1872, 7, 7, 512)\n",
            "Total frame extracted      =  1872\n",
            "Total frame in the vdo     =  1872\n",
            "CPU times: user 24.1 s, sys: 2.29 s, total: 26.4 s\n",
            "Wall time: 26 s\n"
          ]
        }
      ],
      "source": [
        "    %%time\n",
        "    flag =''\n",
        "    image_height  = 224\n",
        "    image_width   = 224\n",
        "    \n",
        "    EXTENSION = \".jpg\"\n",
        "    curr_frame = 0\n",
        "    imageKount = 0\n",
        "    prediction_images = []\n",
        "    \n",
        "    ## Check if VDOFrames is empty or Not --- If empty then frames will be stored else NOT\n",
        "    ## OR we can delete all files in VDOFrames folder unconditionally \n",
        "    \n",
        "    if not os.listdir(FramesVdo):\n",
        "        print (\"empty VDOFrames folder\")\n",
        "        flag = \"empty\"\n",
        "    else:\n",
        "        print (\"Not empty\")\n",
        "    \n",
        "\n",
        "    # Reading the Video File using the VideoCapture Object\n",
        "    video_reader = cv2.VideoCapture(input_video_file_path)\n",
        "\n",
        "    # Getting the width and height of the video \n",
        "    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "\n",
        "\n",
        "    prediction_images = []\n",
        "    while True: \n",
        "\n",
        "        # Reading The Frame\n",
        "        status, frame = video_reader.read() \n",
        "\n",
        "        if not status:\n",
        "            break\n",
        "\n",
        "        \n",
        "        # Resize the Frame to fixed Dimensions\n",
        "        resized_frame = cv2.resize(frame, (image_height, image_width))\n",
        "        \n",
        "        if flag == 'empty':\n",
        "           \n",
        "           temp = Path(input_video_file_path).stem # Find file name without extension\n",
        "           filename =\"/\"+ temp+\"_frame%d.jpg\" % curr_frame\n",
        "           Outimage = os.path.join(FramesVdo+filename)\n",
        "           cv2.imwrite(Outimage, resized_frame)   # save frame as JPEG file\n",
        " \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        resized_frame = image.img_to_array(resized_frame)\n",
        "        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1\n",
        "        normalized_frame = resized_frame / 255\n",
        "\n",
        "        \n",
        "        \n",
        "        # converting the list to numpy array\n",
        "        # appending the image to the image list\n",
        "        prediction_images.append(normalized_frame)\n",
        "        imageKount +=1\n",
        "        curr_frame +=1\n",
        "    ### End of while loop\n",
        "    \n",
        "    \n",
        "    # converting the list to numpy array\n",
        "    prediction_images = np.array(prediction_images)\n",
        "    \n",
        "    # shape of the array\n",
        "    prediction_images.shape\n",
        "    # extracting features for validation frames\n",
        "    prediction_images = base_model.predict(prediction_images)\n",
        "    print('Shape of prediction_images = ' ,prediction_images.shape)\n",
        "    print('Total frame extracted      = ' , curr_frame)\n",
        "    print('Total frame in the vdo     = ' ,imageKount)   \n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21CN2tFgA3D6"
      },
      "outputs": [],
      "source": [
        "# converting features in one dimensional array\n",
        "prediction_images = prediction_images.reshape(prediction_images.shape[0], 7*7*512)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipADDsICA3D6",
        "outputId": "731943ad-3884-46eb-f51d-5ca7dded0d2e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1872, 25088)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# Normalize the array \n",
        "max = prediction_images.max()\n",
        "prediction_images = prediction_images/max\n",
        "prediction_images.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFKmQVzyA3D6"
      },
      "source": [
        "## End of Pre Processing "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xT25GmIA3D6"
      },
      "source": [
        "## Event Prediction "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuDGeXqwA3D7"
      },
      "source": [
        "### Load  model from disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jsjOqbuEA3D7",
        "outputId": "50f1ddb6-fe53-43d2-c9ca-84fc615440ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/MyDrive/Fire_ImageDataSet/ModelSave'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "ModelSaveFolder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3AWd_s3A3D7"
      },
      "outputs": [],
      "source": [
        "model_name = 'Step-1_VGG_model.h5'\n",
        "\n",
        "from tensorflow import keras\n",
        "model = keras.models.load_model(ModelSaveFolder+'/'+model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_FTPsyXA3D7",
        "outputId": "1013d33d-04a0-4476-8f49-2b31a5a10257"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 1 ... 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "predict = []        \n",
        "# predict_classes() function model in order to predict the class values for each instance in the array.\n",
        "#prediction = model.predict(prediction_images)\n",
        "prediction = np.argmax(model.predict(prediction_images), axis=-1)\n",
        "# prediction is an arrary containing frame by frame class prediction - here either Basketball', 'SoccerPenalty'\n",
        "\n",
        "print(prediction)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qhYl99oA3D7"
      },
      "source": [
        "#### Pickle is the standard way of serializing objects in Python.\n",
        "\n",
        "#### Use the pickle operation to serialize  machine learning algorithms and save the serialized format to a file.\n",
        "\n",
        "#### Later we can load this file to deserialize your model and use it to make new predictions.\n",
        "\n",
        "#### Serialization is the process of converting an object into a stream of bytes to store the object or transmit it to memory, or a file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkqnQbr6A3D7"
      },
      "source": [
        "### Save prediction array using pickle dump"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDykgiF3A3D8"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open('Step-2_FireDetection_VGG16_Feature', 'wb') as fp:\n",
        "    pickle.dump(prediction, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_286WIYA3D8"
      },
      "outputs": [],
      "source": [
        "#To read it back:\n",
        "with open ('Step-2_FireDetection_VGG16_Feature', 'rb') as fp:\n",
        "    prediction = pickle.load(fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CitR7y14A3D8",
        "outputId": "fd304134-380c-4d2e-9d9f-67e616551495"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, ..., 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMtsXCzYA3D8"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "of6a8M0WA3D8"
      },
      "source": [
        "### Function To Predict on Live Videos "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rGiTzsaiA3D8",
        "outputId": "dd1ccd57-a600-4591-e816-adc1700696c3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/MyDrive//Fire_ImageDataSet/VDO_fire/VDO-1.mp4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "input_video_file_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k27zoxlQA3D8",
        "outputId": "41088c00-05f2-40b2-f5f9-a89b3faaba60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive//Fire_ImageDataSet/VDO_fire/VDO-1_GR_9_VGG16_FeatureOutput.mp4\n"
          ]
        }
      ],
      "source": [
        "# Constructing The OutputVideo Path\n",
        "output_video_file_path = f'{output_directory}/{video_title}_GR_9_VGG16_FeatureOutput.mp4'\n",
        "\n",
        "print(output_video_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QRzHcibA3D8",
        "outputId": "aeb8679a-b4a6-4722-c5db-1df1dc09a99d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Neutral', 'Fire']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "model_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGna9z0sA3D9",
        "outputId": "90114b3c-327a-4188-d5b9-1333cc4e5269"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "prediction[2]-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "sVKHd_D8A3D9",
        "outputId": "6087c15c-9328-47e5-c811-200dfc938576"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1872\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Neutral'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "x = len(prediction)\n",
        "print(x)\n",
        "# Accessing The Class Name using predicted label.\n",
        "predicted_class_name = model_class[prediction[2]-1]\n",
        "predicted_class_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "iZAhqcbgA3D9",
        "outputId": "f9d34294-6c46-4f67-d4e2-662471f373f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/MyDrive//Fire_ImageDataSet/VDO_fire/VDO-1.mp4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "input_video_file_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oCG5mYw2A3D9",
        "outputId": "b874e9e7-02d3-41e0-b896-c4f13b3afe59"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/MyDrive//Fire_ImageDataSet/VDO_fire/VDO-1_GR_9_VGG16_FeatureOutput.mp4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "output_video_file_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFk5XAV8A3D9"
      },
      "outputs": [],
      "source": [
        "def predict_on_live_video(video_file_path, output_file_path,Algo):\n",
        "\n",
        "    # Initialize a Deque Object with a fixed size which will be used to implement moving/rolling average functionality.\n",
        "    #predicted_labels_probabilities_deque = deque(maxlen = window_size)\n",
        "    \n",
        "    frame_no = -1\n",
        "\n",
        "    # Reading the Video File using the VideoCapture Object\n",
        "    video_reader = cv2.VideoCapture(video_file_path)\n",
        "\n",
        "    # Getting the width and height of the video \n",
        "    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # Writing the Overlayed Video Files Using the VideoWriter Object\n",
        "    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 24, (original_video_width, original_video_height))\n",
        "\n",
        "    while True: \n",
        "\n",
        "        # Reading The Frame\n",
        "        status, frame = video_reader.read() \n",
        "\n",
        "        if not status:\n",
        "            break\n",
        "\n",
        "        frame_no = frame_no+1\n",
        "        \n",
        "        # some frames with black - skip those\n",
        "        ## images in openCV (or in your case frames) are represented as a numpy array, \n",
        "        ## they can be averaged for low values (which represent black frames).\n",
        "        \n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  ## Convert to grey image (black and white)\n",
        "        if np.average(gray) < 20:  ## if it dark screen , skip \n",
        "        # skips an iteration, so the frame isn't saved\n",
        "          continue\n",
        "        \n",
        "\n",
        "           \n",
        "        # Accessing The Class Name using prediction list.\n",
        "        predicted_class_name = model_class[prediction[frame_no]-1]\n",
        "        #print(predicted_class_name)\n",
        "        \n",
        "        \n",
        "        # Overlaying Class Name Text Ontop of the Frame\n",
        "        \n",
        "        cv2.putText(frame, predicted_class_name+'_(' + Algo + ')', (00, 200), cv2.FONT_HERSHEY_DUPLEX, 3.0, (0, 0, 255), 3)\n",
        "\n",
        "        #cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "        #cv2.putText(frame, avg_prob, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
        "        \n",
        "        #cv2.putText(frame, \n",
        "                    #avg_prob, \n",
        "                    #(10, 100),   # bottomLeftCornerOfText\n",
        "                    #cv2.FONT_HERSHEY_SIMPLEX, \n",
        "                    #1, \n",
        "                    #(0, 0, 255), \n",
        "                    #2)\n",
        "                       \n",
        "        # Writing The Frame\n",
        "        video_writer.write(frame)\n",
        "    \n",
        "    # Closing the VideoCapture and VideoWriter objects and releasing all resources held by them. \n",
        "    video_reader.release()\n",
        "    video_writer.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "KdrcuV41A3D-"
      },
      "outputs": [],
      "source": [
        "Model_apply = 'VGG16_Feature'\n",
        "# Calling the predict_on_live_video method to start the Prediction.\n",
        "predict_on_live_video(input_video_file_path, output_video_file_path,Model_apply)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlXKav0AA3D-"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2OzLpTvA3D-"
      },
      "source": [
        "### End"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "PCAMZC321_Group09_FireEventDetection_from_VDO-VGG16_Feature-Gr-9.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "4qhYl99oA3D7",
        "y2OzLpTvA3D-"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7234ff50a8d642009564c62e1a90e3f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VideoModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VideoModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VideoView",
            "autoplay": true,
            "controls": true,
            "format": "mp4",
            "height": "320",
            "layout": "IPY_MODEL_7935f548d115489d9911a6299323ede4",
            "loop": true,
            "width": "320"
          }
        },
        "7935f548d115489d9911a6299323ede4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}