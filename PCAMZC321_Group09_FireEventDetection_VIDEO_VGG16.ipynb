{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/merrymasti015/CAP2022GRP09/blob/main/PCAMZC321_Group09_FireEventDetection_VIDEO_VGG16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwc-vDu57xF7"
      },
      "source": [
        "#Video File detection - addendum to PCAMZC321_Group09_FireEventDetection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpU8Z7PJ43p4"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.layers import Dense, InputLayer, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import math\n",
        "import os\n",
        "from glob import glob\n",
        "from scipy import stats as s\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpLrbcZH799e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d34562a6-c6cf-42ab-a52f-4ea2f974fbf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from IPython import display\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1WHUFdg8LMn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68fe1a08-a1c7-47fe-8184-dece39aef663"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Fire_ImageDataSet/\n"
          ]
        }
      ],
      "source": [
        "RootFolder = \"/content/gdrive/MyDrive/Fire_ImageDataSet/\"\n",
        "print(RootFolder)\n",
        "output_directory = RootFolder+\"/VDO_fire\"  # where is my VDO file ?\n",
        "ModelSaveFolder = RootFolder+\"/ModelSave\" # where is my model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQ8HqBVc9ONh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fe3a148f-b87f-4b49-b050-db91fe1ba943"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/MyDrive/Fire_ImageDataSet//VDO_fire/VDO-1.mp4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "video_title = \"VDO-1\"\n",
        "input_video_file_path = f'{output_directory}/{video_title}.mp4'\n",
        "input_video_file_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYgaqIcv93b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d474e46-8f02-4ecf-ddd4-4ac5eb7a77da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "58900480/58889256 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "## VGG-16 pre-trained model will be used for basemodel\n",
        "# creating the base model of pre-trained VGG16 model\n",
        "base_model = VGG16(weights='imagenet', include_top=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205,
          "referenced_widgets": [
            "51264565382842188e1a4ba1f95a559c",
            "6ac20691056644da95fe0b4d6d7caa46"
          ]
        },
        "id": "YuHsamLD-B6g",
        "outputId": "87daee57-58be-4229-e3c3-d4cdb4e3194d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Video(value=b'\\x00\\x00\\x00 ftypisom\\x00\\x00\\x02\\x00isomiso2avc1mp41\\x00\\x00\\x00\\x08free\\x00\\xe3\\xaa_mdat\\x00\\xâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "51264565382842188e1a4ba1f95a559c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Video.from_file(input_video_file_path)\n",
        "from ipywidgets import Video # Display VDO \n",
        "Video.from_file(input_video_file_path , width = 320 , height = 320)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLaGyLxc_Nre"
      },
      "outputs": [],
      "source": [
        "from ipywidgets import Widget\n",
        "Widget.close_all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SdO_ghy6_Ppf",
        "outputId": "93036570-14c0-4ee9-c6e3-8d3e156b6190"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/MyDrive/Fire_ImageDataSet//VDO_fire/VDOFrames'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "FramesVdo  = output_directory+\"/VDOFrames\"\n",
        "if not os.path.exists(FramesVdo):\n",
        "   os.makedirs(FramesVdo) \n",
        "FramesVdo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFQlZFqb_oZ2",
        "outputId": "ab85d88f-9a4f-42b3-f697-23a39f4c3f1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fps = 25\n",
            "number of frames = 1876\n",
            "duration (S) = 75\n",
            "duration (M:S) = 1:15\n"
          ]
        }
      ],
      "source": [
        "# Reading the Video File using the VideoCapture Object\n",
        "video_reader = cv2.VideoCapture(input_video_file_path)\n",
        "# Set frames-per-second for capture\n",
        "fps = round(video_reader.get(cv2.CAP_PROP_FPS))\n",
        "## Total frame count in the VDO \n",
        "frame_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT)) -1 \n",
        "\n",
        "\n",
        "duration = round(frame_count/fps)\n",
        "\n",
        "print('fps = ' + str(fps))\n",
        "print('number of frames = ' + str(frame_count))\n",
        "print('duration (S) = ' + str(duration))\n",
        "\n",
        "minutes = int(duration/60)\n",
        "seconds = duration%60\n",
        "print('duration (M:S) = ' + str(minutes) + ':' + str(seconds))  ## In Minute and seconds\n",
        "\n",
        "video_reader.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugz-QMI5_ryO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d46e3016-e0a7-4437-9e20-a704f818da50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "empty VDOFrames folder\n",
            "Shape of prediction_images =  (1872, 7, 7, 512)\n",
            "Total frame extracted      =  1872\n",
            "Total frame in the vdo     =  1872\n",
            "CPU times: user 30.3 s, sys: 2.57 s, total: 32.8 s\n",
            "Wall time: 38.1 s\n"
          ]
        }
      ],
      "source": [
        "    %%time\n",
        "    flag =''\n",
        "    image_height  = 224\n",
        "    image_width   = 224\n",
        "    \n",
        "    EXTENSION = \".jpg\"\n",
        "    curr_frame = 0\n",
        "    imageKount = 0\n",
        "    prediction_images = []\n",
        "    \n",
        "    ## Check if VDOFrames is empty or Not --- If empty then frames will be stored else NOT\n",
        "    ## OR we can delete all files in VDOFrames folder unconditionally \n",
        "    \n",
        "    if not os.listdir(FramesVdo):\n",
        "        print (\"empty VDOFrames folder\")\n",
        "        flag = \"empty\"\n",
        "    else:\n",
        "        print (\"Not empty\")\n",
        "    \n",
        "\n",
        "    # Reading the Video File using the VideoCapture Object\n",
        "    video_reader = cv2.VideoCapture(input_video_file_path)\n",
        "\n",
        "    # Getting the width and height of the video \n",
        "    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "\n",
        "\n",
        "    prediction_images = []\n",
        "    while True: \n",
        "\n",
        "        # Reading The Frame\n",
        "        status, frame = video_reader.read() \n",
        "\n",
        "        if not status:\n",
        "            break\n",
        "\n",
        "        \n",
        "        # Resize the Frame to fixed Dimensions\n",
        "        resized_frame = cv2.resize(frame, (image_height, image_width))\n",
        "        \n",
        "        if flag == 'empty':\n",
        "           \n",
        "           temp = Path(input_video_file_path).stem # Find file name without extension\n",
        "           filename =\"/\"+ temp+\"_frame%d.jpg\" % curr_frame\n",
        "           Outimage = os.path.join(FramesVdo+filename)\n",
        "           cv2.imwrite(Outimage, resized_frame)   # save frame as JPEG file\n",
        " \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        resized_frame = image.img_to_array(resized_frame)\n",
        "        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1\n",
        "        normalized_frame = resized_frame / 255\n",
        "\n",
        "        \n",
        "        \n",
        "        # converting the list to numpy array\n",
        "        # appending the image to the image list\n",
        "        prediction_images.append(normalized_frame)\n",
        "        imageKount +=1\n",
        "        curr_frame +=1\n",
        "    ### End of while loop\n",
        "    \n",
        "    \n",
        "    # converting the list to numpy array\n",
        "    prediction_images = np.array(prediction_images)\n",
        "    \n",
        "    # shape of the array\n",
        "    prediction_images.shape\n",
        "    # extracting features for validation frames\n",
        "    prediction_images = base_model.predict(prediction_images)\n",
        "    print('Shape of prediction_images = ' ,prediction_images.shape)\n",
        "    print('Total frame extracted      = ' , curr_frame)\n",
        "    print('Total frame in the vdo     = ' ,imageKount)   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIHKyNx1Dqm9"
      },
      "outputs": [],
      "source": [
        "# converting features in one dimensional array\n",
        "prediction_images = prediction_images.reshape(prediction_images.shape[0], 7*7*512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTvXIS5tDtqT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ca5f8d3-2390-4921-8bcf-1213952ce22f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1872, 25088)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Normalize the array \n",
        "max = prediction_images.max()\n",
        "prediction_images = prediction_images/max\n",
        "prediction_images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9Az_3HFD4th"
      },
      "outputs": [],
      "source": [
        "#Load Model\n",
        "from tensorflow import keras\n",
        "\n",
        "model_name = 'Step-1_VGG_model.h5'\n",
        "\n",
        "model = keras.models.load_model(ModelSaveFolder+'/'+model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDF4nqT_D5vD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "332a6bce-c581-4b04-955c-94c8dc67dbfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 1 ... 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "predict = []        \n",
        "# predict_classes() function model in order to predict the class values for each instance in the array.\n",
        "prediction = np.argmax(model.predict(prediction_images), axis=-1)\n",
        "# prediction is an arrary containing frame by frame class prediction - here either Basketball', 'SoccerPenalty'\n",
        "\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TILxghvEiCK"
      },
      "source": [
        "#### Pickle is the standard way of serializing objects in Python.\n",
        "\n",
        "#### Use the pickle operation to serialize  machine learning algorithms and save the serialized format to a file.\n",
        "\n",
        "#### Later we can load this file to deserialize your model and use it to make new predictions.\n",
        "\n",
        "#### Serialization is the process of converting an object into a stream of bytes to store the object or transmit it to memory, or a file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jY5sjrsFEZTu"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open('Step-2_FireDetection_VGG16_Feature', 'wb') as fp:\n",
        "    pickle.dump(prediction, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6RKqcmSErAU"
      },
      "outputs": [],
      "source": [
        "#To read it back:\n",
        "with open ('Step-2_FireDetection_VGG16_Feature', 'rb') as fp:\n",
        "    prediction = pickle.load(fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zf-0IrIeEtIB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19b90257-5a1c-4ff7-f21e-7f6f06ca0d1b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, ..., 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmPDaSyTEyVb"
      },
      "source": [
        "### Function To Predict on Live Videos "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5m1FgjJEvAr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c22cd407-2e43-4d8f-b24d-8ab61b435186"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/MyDrive/Fire_ImageDataSet//VDO_fire/VDO-1.mp4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "input_video_file_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0BQ59WBE4kj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "040f9b2f-78b6-4495-8ad0-602b41afcee0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Fire_ImageDataSet//VDO_fire/VDO-1_VGG16_FeatureOutput.mp4\n"
          ]
        }
      ],
      "source": [
        "# Constructing The OutputVideo Path\n",
        "output_video_file_path = f'{output_directory}/{video_title}_VGG16_FeatureOutput.mp4'\n",
        "\n",
        "print(output_video_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7C4rJ8dE6uQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af8e9e24-e0e9-4d58-db91-52f9605e2ef1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Neutral', 'Fire']\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "trainFrames  = pd.read_csv(RootFolder+\"/OutputFiles/Train_FramePathLocation.csv\")\n",
        "\n",
        "trainFrames.head()\n",
        "# Get the unique values of 'class' column - How many distinct classes\n",
        "model_class = trainFrames['class'].unique().tolist()\n",
        "print(model_class)\n",
        "model_output_size = len(model_class)\n",
        "print(model_output_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSj9rmiSFeJc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7f6e951-7e64-493a-c1b9-ef4f217ab089"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "prediction[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOwhWOe2FZQd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "a19fbeab-6a42-4d8c-90eb-7b65438f2270"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1872\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Fire'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "x = len(prediction)\n",
        "print(x)\n",
        "# Accessing The Class Name using predicted label.\n",
        "predicted_class_name = model_class[prediction[2]]\n",
        "predicted_class_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zVvk8rAFsys",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1159554-2293-4556-c05a-2d84ad0912eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input path:/content/gdrive/MyDrive/Fire_ImageDataSet//VDO_fire/VDO-1.mp4\n",
            "Output path:/content/gdrive/MyDrive/Fire_ImageDataSet//VDO_fire/VDO-1_VGG16_FeatureOutput.mp4\n"
          ]
        }
      ],
      "source": [
        "print('Input path:'+input_video_file_path)\n",
        "print('Output path:'+output_video_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_SkptD1F15J"
      },
      "outputs": [],
      "source": [
        "def predict_on_live_video(video_file_path, output_file_path,Algo):\n",
        "\n",
        "    # Initialize a Deque Object with a fixed size which will be used to implement moving/rolling average functionality.\n",
        "    #predicted_labels_probabilities_deque = deque(maxlen = window_size)\n",
        "    \n",
        "    frame_no = -1\n",
        "\n",
        "    # Reading the Video File using the VideoCapture Object\n",
        "    video_reader = cv2.VideoCapture(video_file_path)\n",
        "\n",
        "    # Getting the width and height of the video \n",
        "    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # Writing the Overlayed Video Files Using the VideoWriter Object\n",
        "    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('M', 'P', '4', 'V'), 24, (original_video_width, original_video_height))\n",
        "\n",
        "    while True: \n",
        "\n",
        "        # Reading The Frame\n",
        "        status, frame = video_reader.read() \n",
        "\n",
        "        if not status:\n",
        "            break\n",
        "\n",
        "        frame_no = frame_no+1\n",
        "        \n",
        "        # some frames with black - skip those\n",
        "        ## images in openCV (or in your case frames) are represented as a numpy array, \n",
        "        ## they can be averaged for low values (which represent black frames).\n",
        "        \n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  ## Convert to grey image (black and white)\n",
        "        if np.average(gray) < 20:  ## if it dark screen , skip \n",
        "        # skips an iteration, so the frame isn't saved\n",
        "          continue\n",
        "        \n",
        "\n",
        "           \n",
        "        # Accessing The Class Name using prediction list.\n",
        "        predicted_class_name = model_class[prediction[frame_no]]\n",
        "        #print(predicted_class_name)\n",
        "        \n",
        "        \n",
        "        # Overlaying Class Name Text Ontop of the Frame\n",
        "        \n",
        "        cv2.putText(frame, predicted_class_name+'_(' + Algo + ')', (00, 200), cv2.FONT_HERSHEY_DUPLEX, 3.0, (0, 0, 255), 3)\n",
        "\n",
        "        #cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "        #cv2.putText(frame, avg_prob, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
        "        \n",
        "        #cv2.putText(frame, \n",
        "                    #avg_prob, \n",
        "                    #(10, 100),   # bottomLeftCornerOfText\n",
        "                    #cv2.FONT_HERSHEY_SIMPLEX, \n",
        "                    #1, \n",
        "                    #(0, 0, 255), \n",
        "                    #2)\n",
        "                       \n",
        "        # Writing The Frame\n",
        "        video_writer.write(frame)\n",
        "    \n",
        "    # Closing the VideoCapture and VideoWriter objects and releasing all resources held by them. \n",
        "    video_reader.release()\n",
        "    video_writer.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9Tp_uNEF7Lw"
      },
      "outputs": [],
      "source": [
        "Model_apply = 'VGG16_Feature'\n",
        "# Calling the predict_on_live_video method to start the Prediction.\n",
        "predict_on_live_video(input_video_file_path, output_video_file_path,Model_apply)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "PCAMZC321_Group09_FireEventDetection_VIDEO_VGG16.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPQxDur6JpZ5La9LHB3RVm+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "51264565382842188e1a4ba1f95a559c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VideoModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VideoModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VideoView",
            "autoplay": true,
            "controls": true,
            "format": "mp4",
            "height": "320",
            "layout": "IPY_MODEL_6ac20691056644da95fe0b4d6d7caa46",
            "loop": true,
            "width": "320"
          }
        },
        "6ac20691056644da95fe0b4d6d7caa46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}